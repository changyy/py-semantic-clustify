#!/usr/bin/env python3
"""
ç®¡é“æ•´åˆæ¼”ç¤ºè…³æœ¬
å±•ç¤ºå¦‚ä½•åœ¨å¯¦éš›è³‡æ–™ç®¡é“ä¸­ä½¿ç”¨ä¸åŒçš„è¼¸å‡ºæ ¼å¼
"""

import json
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Any


def create_large_dataset() -> str:
    """å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†"""
    documents = []
    
    # ç§‘æŠ€é¡æ–‡æª”
    tech_docs = [
        "Artificial intelligence and machine learning algorithms",
        "Deep neural networks and backpropagation",
        "Computer vision and image recognition",
        "Natural language processing techniques",
        "Reinforcement learning in robotics",
        "Quantum computing fundamentals",
        "Blockchain technology applications",
        "Cloud computing architecture",
        "Data structures and algorithms",
        "Software engineering best practices"
    ]
    
    # å•†æ¥­é¡æ–‡æª”
    business_docs = [
        "Financial markets and investment strategies",
        "Marketing automation and customer segmentation",
        "Supply chain management optimization",
        "Digital transformation initiatives",
        "Business intelligence and analytics",
        "Project management methodologies",
        "Human resources and talent acquisition",
        "Strategic planning and execution",
        "Risk management frameworks",
        "Corporate governance principles"
    ]
    
    # ç§‘å­¸é¡æ–‡æª”
    science_docs = [
        "Climate change and environmental science",
        "Biotechnology and genetic engineering",
        "Physics research and quantum mechanics",
        "Chemistry and molecular biology",
        "Medical research and drug discovery",
        "Renewable energy technologies",
        "Space exploration and astronomy",
        "Mathematical modeling and statistics",
        "Geological surveys and earth science",
        "Environmental conservation strategies"
    ]
    
    # ç‚ºæ¯å€‹é¡åˆ¥ç”Ÿæˆå¸¶æœ‰ç›¸ä¼¼å‘é‡çš„æ–‡æª”
    import random
    import numpy as np
    
    def generate_embedding(base_vector: List[float], noise_level: float = 0.1) -> List[float]:
        """ç”Ÿæˆå…·æœ‰å™ªè²çš„ç›¸ä¼¼å‘é‡"""
        return [val + random.uniform(-noise_level, noise_level) for val in base_vector]
    
    # ç§‘æŠ€é¡åˆ¥åŸºå‘é‡
    tech_base = [0.8, 0.7, 0.6, 0.2, 0.1]
    for i, doc in enumerate(tech_docs):
        documents.append({
            "title": doc,
            "category": "Technology",
            "document_id": f"tech_{i:03d}",
            "embedding": generate_embedding(tech_base)
        })
    
    # å•†æ¥­é¡åˆ¥åŸºå‘é‡
    business_base = [0.2, 0.3, 0.8, 0.7, 0.6]
    for i, doc in enumerate(business_docs):
        documents.append({
            "title": doc,
            "category": "Business",
            "document_id": f"biz_{i:03d}",
            "embedding": generate_embedding(business_base)
        })
    
    # ç§‘å­¸é¡åˆ¥åŸºå‘é‡
    science_base = [0.1, 0.8, 0.2, 0.9, 0.7]
    for i, doc in enumerate(science_docs):
        documents.append({
            "title": doc,
            "category": "Science",
            "document_id": f"sci_{i:03d}",
            "embedding": generate_embedding(science_base)
        })
    
    # ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for doc in documents:
            json.dump(doc, f, ensure_ascii=False)
            f.write('\n')
        return f.name


def pipeline_step_1_basic_clustering(input_file: str) -> str:
    """ç®¡é“æ­¥é©Ÿ 1: åŸºç¤èšé¡"""
    print("ğŸ”„ ç®¡é“æ­¥é©Ÿ 1: åŸ·è¡ŒåŸºç¤èšé¡...")
    
    output_file = "pipeline_step1_clusters.jsonl"
    cmd = [
        "semantic-clustify",
        "--input", input_file,
        "--embedding-field", "embedding",
        "--method", "kmeans",
        "--n-clusters", "auto",
        "--output-format", "enriched-labeled",
        "--output", output_file
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"âŒ æ­¥é©Ÿ 1 å¤±æ•—: {result.stderr}")
        return None
    
    print(f"âœ… æ­¥é©Ÿ 1 å®Œæˆ: {output_file}")
    return output_file


def pipeline_step_2_filter_large_clusters(input_file: str) -> str:
    """ç®¡é“æ­¥é©Ÿ 2: ç¯©é¸å¤§å‹èšé¡"""
    print("ğŸ” ç®¡é“æ­¥é©Ÿ 2: ç¯©é¸å¤§å‹èšé¡ï¼ˆcluster_size >= 8ï¼‰...")
    
    large_cluster_docs = []
    cluster_stats = {}
    
    with open(input_file, 'r') as f:
        for line in f:
            if line.strip():
                doc = json.loads(line)
                cluster_size = doc.get("cluster_size", 0)
                cluster_id = doc.get("cluster_id", -1)
                
                # çµ±è¨ˆèšé¡ä¿¡æ¯
                if cluster_id not in cluster_stats:
                    cluster_stats[cluster_id] = {
                        "size": cluster_size,
                        "documents": 0,
                        "categories": set()
                    }
                
                cluster_stats[cluster_id]["documents"] += 1
                cluster_stats[cluster_id]["categories"].add(doc.get("category", "Unknown"))
                
                # ç¯©é¸å¤§å‹èšé¡
                if cluster_size >= 8:
                    large_cluster_docs.append(doc)
    
    # ä¿å­˜ç¯©é¸çµæœ
    output_file = "pipeline_step2_large_clusters.jsonl"
    with open(output_file, 'w') as f:
        for doc in large_cluster_docs:
            json.dump(doc, f, ensure_ascii=False)
            f.write('\n')
    
    print(f"âœ… æ­¥é©Ÿ 2 å®Œæˆ: å¾ {len(cluster_stats)} å€‹èšé¡ä¸­ç¯©é¸å‡º {len(large_cluster_docs)} å€‹å¤§å‹èšé¡æ–‡æª”")
    print(f"   èšé¡çµ±è¨ˆ:")
    for cluster_id, stats in cluster_stats.items():
        categories_str = ", ".join(stats["categories"])
        print(f"     èšé¡ {cluster_id}: {stats['documents']} å€‹æ–‡æª”, é¡åˆ¥: {categories_str}")
    
    return output_file


def pipeline_step_3_refined_clustering(input_file: str) -> str:
    """ç®¡é“æ­¥é©Ÿ 3: å°å¤§å‹èšé¡é€²è¡Œç´°åŒ–"""
    print("ğŸ”¬ ç®¡é“æ­¥é©Ÿ 3: å°å¤§å‹èšé¡é€²è¡Œç´°åŒ–èšé¡...")
    
    output_file = "pipeline_step3_refined_clusters.jsonl"
    cmd = [
        "semantic-clustify",
        "--input", input_file,
        "--embedding-field", "embedding",
        "--method", "hierarchical",
        "--n-clusters", "auto",
        "--output-format", "streaming-grouped",
        "--output", output_file
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"âŒ æ­¥é©Ÿ 3 å¤±æ•—: {result.stderr}")
        return None
    
    print(f"âœ… æ­¥é©Ÿ 3 å®Œæˆ: {output_file}")
    return output_file


def analyze_pipeline_results(streaming_file: str):
    """åˆ†æç®¡é“çµæœ"""
    print("ğŸ“Š åˆ†æç®¡é“çµæœ...")
    
    metadata = None
    clusters = []
    summary = None
    
    with open(streaming_file, 'r') as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                data_type = data.get("type")
                
                if data_type == "clustering_metadata":
                    metadata = data
                elif data_type == "cluster":
                    clusters.append(data)
                elif data_type == "clustering_summary":
                    summary = data
    
    print(f"\nğŸ“ˆ æœ€çµ‚çµæœ:")
    print(f"   èšé¡æ–¹æ³•: {metadata.get('method', 'N/A')}")
    print(f"   èšé¡æ•¸é‡: {len(clusters)}")
    print(f"   ç¸½æ–‡æª”æ•¸: {summary.get('total_documents', 'N/A')}")
    print(f"   è¼ªå»“ä¿‚æ•¸: {summary.get('silhouette_score', 0):.3f}" if summary.get('silhouette_score') else "   è¼ªå»“ä¿‚æ•¸: N/A")
    
    print(f"\nğŸ“‹ å„èšé¡è©³æƒ…:")
    for cluster in clusters:
        cluster_id = cluster.get("cluster_id", -1)
        size = cluster.get("size", 0)
        density = cluster.get("density", 0)
        
        # åˆ†æèšé¡ä¸­çš„é¡åˆ¥åˆ†å¸ƒ
        categories = {}
        documents = cluster.get("documents", [])
        for doc in documents:
            cat = doc.get("category", "Unknown")
            categories[cat] = categories.get(cat, 0) + 1
        
        category_str = ", ".join([f"{cat}({count})" for cat, count in categories.items()])
        print(f"   èšé¡ {cluster_id}: {size} å€‹æ–‡æª”, å¯†åº¦ {density:.3f}, é¡åˆ¥åˆ†å¸ƒ: {category_str}")


def demonstrate_streaming_processing(streaming_file: str):
    """æ¼”ç¤ºä¸²æµè™•ç†èƒ½åŠ›"""
    print("\nğŸš° æ¼”ç¤ºä¸²æµè™•ç†èƒ½åŠ›...")
    
    # æ¨¡æ“¬ä¸²æµè™•ç†ï¼šé€è¡Œè®€å–ä¸¦è™•ç†
    print("   ğŸ”„ æ¨¡æ“¬å¯¦æ™‚è™•ç† streaming-grouped æ ¼å¼...")
    
    processed_clusters = 0
    processed_documents = 0
    
    with open(streaming_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                data = json.loads(line)
                data_type = data.get("type")
                
                if data_type == "clustering_metadata":
                    print(f"   ğŸ“‹ è¡Œ {line_num}: æ¥æ”¶åˆ°å…ƒæ•¸æ“š - æ–¹æ³•: {data.get('method')}")
                elif data_type == "cluster":
                    cluster_id = data.get("cluster_id", -1)
                    size = data.get("size", 0)
                    processed_clusters += 1
                    processed_documents += size
                    print(f"   ğŸ“¦ è¡Œ {line_num}: è™•ç†èšé¡ {cluster_id} - {size} å€‹æ–‡æª”")
                elif data_type == "clustering_summary":
                    print(f"   ğŸ“Š è¡Œ {line_num}: æ¥æ”¶åˆ°æ‘˜è¦ - ç¸½è¨ˆ: {data.get('total_documents')} å€‹æ–‡æª”")
    
    print(f"   âœ… ä¸²æµè™•ç†å®Œæˆ: è™•ç†äº† {processed_clusters} å€‹èšé¡ï¼Œ{processed_documents} å€‹æ–‡æª”")


def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºå®Œæ•´ç®¡é“"""
    print("ğŸ¯ è³‡æ–™ç®¡é“æ•´åˆæ¼”ç¤º")
    print("=" * 50)
    
    try:
        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        print("ğŸ“ å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†...")
        input_file = create_large_dataset()
        print(f"âœ… å‰µå»ºäº†åŒ…å« 30 å€‹æ–‡æª”çš„æ¸¬è©¦æ•¸æ“š: {input_file}")
        
        # ç®¡é“æ­¥é©Ÿ 1: åŸºç¤èšé¡
        step1_output = pipeline_step_1_basic_clustering(input_file)
        if not step1_output:
            return
        
        # ç®¡é“æ­¥é©Ÿ 2: ç¯©é¸å¤§å‹èšé¡
        step2_output = pipeline_step_2_filter_large_clusters(step1_output)
        if not step2_output:
            return
        
        # ç®¡é“æ­¥é©Ÿ 3: ç´°åŒ–èšé¡
        step3_output = pipeline_step_3_refined_clustering(step2_output)
        if not step3_output:
            return
        
        # åˆ†ææœ€çµ‚çµæœ
        analyze_pipeline_results(step3_output)
        
        # æ¼”ç¤ºä¸²æµè™•ç†
        demonstrate_streaming_processing(step3_output)
        
        print(f"\nğŸ¯ ç®¡é“æ•´åˆå„ªå‹¢:")
        print("â€¢ ğŸ”„ enriched-labeled æ ¼å¼æ”¯æ´åŸºæ–¼çµ±è¨ˆçš„ç¯©é¸")
        print("â€¢ ğŸš° streaming-grouped æ ¼å¼æ”¯æ´çµæ§‹åŒ–ä¸²æµè™•ç†")
        print("â€¢ ğŸ“Š è±å¯Œçš„å…ƒæ•¸æ“šä¾¿æ–¼ç®¡é“ç›£æ§å’Œèª¿è©¦")
        print("â€¢ ğŸ’¾ è¨˜æ†¶é«”æ•ˆç‡é«˜ï¼Œé©åˆå¤§è¦æ¨¡æ•¸æ“šè™•ç†")
        print("â€¢ ğŸ”— æ˜“æ–¼èˆ‡å…¶ä»–å·¥å…·å’Œæ¡†æ¶æ•´åˆ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¼”ç¤ºè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        for filename in [input_file, "pipeline_step1_clusters.jsonl", 
                        "pipeline_step2_large_clusters.jsonl", 
                        "pipeline_step3_refined_clusters.jsonl"]:
            if filename and Path(filename).exists():
                Path(filename).unlink(missing_ok=True)
        
        print(f"\nâœ¨ ç®¡é“æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
