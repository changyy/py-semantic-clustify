#!/usr/bin/env python3
"""
ä¸²æ¥æ ¼å¼æ¼”ç¤ºè…³æœ¬
å±•ç¤ºä¸åŒè¼¸å‡ºæ ¼å¼çš„ç‰¹é»å’Œä¸²æ¥æ©Ÿåˆ¶çš„å„ªå‹¢
"""

import json
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Any


def create_demo_data() -> str:
    """å‰µå»ºæ¼”ç¤ºæ•¸æ“š"""
    demo_data = []
    
    # æ©Ÿå™¨å­¸ç¿’ç›¸é—œæ–‡æª”
    ml_docs = [
        {"title": "æ©Ÿå™¨å­¸ç¿’åŸºç¤", "content": "ä»‹ç´¹æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µ", "category": "ML", "embedding": [0.1, 0.2, 0.3, 0.4, 0.5]},
        {"title": "æ·±åº¦å­¸ç¿’å…¥é–€", "content": "ç¥ç¶“ç¶²çµ¡åŸºç¤", "category": "ML", "embedding": [0.12, 0.22, 0.32, 0.42, 0.52]},
        {"title": "ç›£ç£å­¸ç¿’æ–¹æ³•", "content": "åˆ†é¡å’Œå›æ­¸ç®—æ³•", "category": "ML", "embedding": [0.08, 0.18, 0.28, 0.38, 0.48]},
    ]
    
    # æ•¸æ“šç§‘å­¸ç›¸é—œæ–‡æª”
    ds_docs = [
        {"title": "æ•¸æ“šåˆ†ææŠ€è¡“", "content": "çµ±è¨ˆåˆ†ææ–¹æ³•", "category": "DS", "embedding": [0.6, 0.1, 0.2, 0.3, 0.4]},
        {"title": "æ•¸æ“šå¯è¦–åŒ–", "content": "åœ–è¡¨å’Œå„€è¡¨æ¿", "category": "DS", "embedding": [0.62, 0.12, 0.22, 0.32, 0.42]},
        {"title": "å¤§æ•¸æ“šè™•ç†", "content": "åˆ†å¸ƒå¼è¨ˆç®—", "category": "DS", "embedding": [0.58, 0.08, 0.18, 0.28, 0.38]},
    ]
    
    # ç¨‹å¼è¨­è¨ˆç›¸é—œæ–‡æª”  
    prog_docs = [
        {"title": "Python ç¨‹å¼è¨­è¨ˆ", "content": "Python èªæ³•åŸºç¤", "category": "Programming", "embedding": [0.9, 0.1, 0.05, 0.02, 0.03]},
        {"title": "JavaScript é–‹ç™¼", "content": "å‰ç«¯é–‹ç™¼æŠ€è¡“", "category": "Programming", "embedding": [0.92, 0.12, 0.07, 0.04, 0.05]},
    ]
    
    demo_data.extend(ml_docs)
    demo_data.extend(ds_docs) 
    demo_data.extend(prog_docs)
    
    # ä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        for doc in demo_data:
            json.dump(doc, f, ensure_ascii=False)
            f.write('\n')
        return f.name


def run_clustering(input_file: str, output_format: str) -> str:
    """åŸ·è¡Œèšé¡ä¸¦è¿”å›è¼¸å‡ºæ–‡ä»¶è·¯å¾„"""
    output_file = f"demo_output_{output_format.replace('-', '_')}.jsonl"
    
    cmd = [
        "semantic-clustify",
        "--input", input_file,
        "--embedding-field", "embedding", 
        "--method", "kmeans",
        "--n-clusters", "3",
        "--output-format", output_format,
        "--output", output_file
    ]
    
    print(f"ğŸ”„ åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ éŒ¯èª¤: {result.stderr}")
        return None
        
    print(f"âœ… æˆåŠŸç”Ÿæˆ: {output_file}")
    return output_file


def analyze_format(file_path: str, format_name: str) -> Dict[str, Any]:
    """åˆ†æè¼¸å‡ºæ ¼å¼ç‰¹æ€§"""
    if not Path(file_path).exists():
        return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
    file_size = Path(file_path).stat().st_size
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    analysis = {
        "format": format_name,
        "file_size": file_size,
        "line_count": len(content.strip().split('\n')),
        "streaming_friendly": format_name in ["labeled", "enriched-labeled", "streaming-grouped"],
        "memory_efficient": format_name != "grouped"
    }
    
    # æ ¼å¼ç‰¹å®šåˆ†æ
    if format_name == "grouped":
        try:
            data = json.loads(content)
            analysis["clusters"] = len(data)
            analysis["structure"] = "JSON array of arrays"
        except:
            analysis["parse_error"] = True
    
    elif format_name in ["labeled", "enriched-labeled"]:
        lines = [line for line in content.strip().split('\n') if line.strip()]
        analysis["document_lines"] = len(lines)
        analysis["structure"] = "JSONL documents with cluster_id"
        
        if format_name == "enriched-labeled":
            # æª¢æŸ¥æ˜¯å¦æœ‰é™„åŠ å…ƒæ•¸æ“š
            try:
                first_doc = json.loads(lines[0])
                analysis["has_cluster_stats"] = "cluster_size" in first_doc
            except:
                pass
    
    elif format_name == "streaming-grouped":
        lines = [line for line in content.strip().split('\n') if line.strip()]
        analysis["total_lines"] = len(lines)
        analysis["structure"] = "JSONL with metadata, clusters, and summary"
        
        # åˆ†æå„ç¨®è¡Œé¡å‹
        line_types = {}
        for line in lines:
            try:
                data = json.loads(line)
                line_type = data.get("type", "unknown")
                line_types[line_type] = line_types.get(line_type, 0) + 1
            except:
                pass
        analysis["line_types"] = line_types
    
    return analysis


def demonstrate_streaming_processing(file_path: str, format_name: str):
    """æ¼”ç¤ºä¸²æµè™•ç†èƒ½åŠ›"""
    print(f"\nğŸ”„ æ¼”ç¤º {format_name} æ ¼å¼çš„ä¸²æµè™•ç†:")
    
    if format_name == "grouped":
        print("   âŒ éœ€è¦å…¨éƒ¨åŠ è¼‰åˆ°è¨˜æ†¶é«”ä¸­æ‰èƒ½è™•ç†")
        with open(file_path, 'r') as f:
            data = json.load(f)
            print(f"   ğŸ“Š åŠ è¼‰äº† {len(data)} å€‹ç¾¤çµ„åˆ°è¨˜æ†¶é«”")
    
    elif format_name == "labeled":
        print("   âœ… å¯ä»¥é€è¡Œè™•ç†ï¼Œè¨˜æ†¶é«”æ•ˆç‡é«˜")
        line_count = 0
        cluster_counts = {}
        
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    doc = json.loads(line)
                    cluster_id = doc.get("cluster_id", -1)
                    cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1
                    line_count += 1
        
        print(f"   ğŸ“Š ä¸²æµè™•ç†äº† {line_count} è¡Œï¼Œç™¼ç¾ {len(cluster_counts)} å€‹ç¾¤çµ„")
    
    elif format_name == "enriched-labeled":
        print("   âœ… å¯ä»¥é€è¡Œè™•ç†ï¼Œä¸”æ¯è¡ŒåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡")
        large_clusters = []
        
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    doc = json.loads(line)
                    if doc.get("cluster_size", 0) >= 3:  # åªè™•ç†å¤§ç¾¤çµ„
                        large_clusters.append(doc)
        
        print(f"   ğŸ“Š ä¸²æµç¯©é¸å‡º {len(large_clusters)} å€‹å±¬æ–¼å¤§ç¾¤çµ„çš„æ–‡æª”")
    
    elif format_name == "streaming-grouped":
        print("   âœ… çµæ§‹åŒ–ä¸²æµè™•ç†ï¼Œé©åˆç®¡é“é›†æˆ")
        metadata = None
        cluster_count = 0
        
        with open(file_path, 'r') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    if data.get("type") == "clustering_metadata":
                        metadata = data
                    elif data.get("type") == "cluster":
                        cluster_count += 1
        
        print(f"   ğŸ“Š å…ƒæ•¸æ“š: {metadata.get('method')} æ–¹æ³•ï¼Œ{cluster_count} å€‹ç¾¤çµ„")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ èªç¾©èšé¡ä¸²æ¥æ ¼å¼æ¼”ç¤º")
    print("=" * 50)
    
    # å‰µå»ºæ¼”ç¤ºæ•¸æ“š
    print("\nğŸ“ å‰µå»ºæ¼”ç¤ºæ•¸æ“š...")
    input_file = create_demo_data()
    print(f"âœ… å‰µå»ºäº†åŒ…å« 8 å€‹æ–‡æª”çš„æ¸¬è©¦æ•¸æ“š: {input_file}")
    
    # æ¸¬è©¦æ‰€æœ‰æ ¼å¼
    formats = ["grouped", "labeled", "enriched-labeled", "streaming-grouped"]
    results = {}
    
    for format_name in formats:
        print(f"\nğŸ§ª æ¸¬è©¦ {format_name} æ ¼å¼:")
        output_file = run_clustering(input_file, format_name)
        
        if output_file:
            analysis = analyze_format(output_file, format_name)
            results[format_name] = {
                "file": output_file,
                "analysis": analysis
            }
            
            # é¡¯ç¤ºåˆ†æçµæœ
            print(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {analysis['file_size']} bytes")
            print(f"   ğŸ“„ è¡Œæ•¸: {analysis['line_count']}")
            print(f"   ğŸš° ä¸²æµå‹å–„: {'âœ…' if analysis['streaming_friendly'] else 'âŒ'}")
            print(f"   ğŸ’¾ è¨˜æ†¶é«”æ•ˆç‡: {'âœ…' if analysis['memory_efficient'] else 'âŒ'}")
            
            # æ¼”ç¤ºä¸²æµè™•ç†
            demonstrate_streaming_processing(output_file, format_name)
    
    # ç¸½çµæ¯”è¼ƒ
    print(f"\nğŸ“Š æ ¼å¼æ¯”è¼ƒç¸½çµ:")
    print("=" * 50)
    
    print(f"{'æ ¼å¼':<20} {'æ–‡ä»¶å¤§å°':<10} {'ä¸²æµå‹å–„':<8} {'é©ç”¨å ´æ™¯'}")
    print("-" * 60)
    
    scenarios = {
        "grouped": "å°è¦æ¨¡å¯¦é©—åˆ†æ",
        "labeled": "åŸºç¤ç®¡é“è™•ç†", 
        "enriched-labeled": "ä¸Šä¸‹æ–‡è±å¯Œçš„ç®¡é“",
        "streaming-grouped": "çµæ§‹åŒ–å¤§è¦æ¨¡ç®¡é“"
    }
    
    for format_name in formats:
        if format_name in results:
            analysis = results[format_name]["analysis"]
            size = analysis["file_size"]
            streaming = "âœ…" if analysis["streaming_friendly"] else "âŒ"
            scenario = scenarios.get(format_name, "æœªçŸ¥")
            print(f"{format_name:<20} {size:<10} {streaming:<8} {scenario}")
    
    print(f"\nğŸ¯ å»ºè­°ä½¿ç”¨ç­–ç•¥:")
    print("â€¢ ğŸ§ª å¯¦é©—éšæ®µ: ä½¿ç”¨ grouped æ ¼å¼ï¼Œä¾¿æ–¼åˆ†æç¾¤çµ„çµæ§‹")
    print("â€¢ ğŸ”„ ç”Ÿç”¢ç®¡é“: ä½¿ç”¨ enriched-labeledï¼Œå¹³è¡¡æ•ˆç‡å’Œä¿¡æ¯å®Œæ•´æ€§")
    print("â€¢ ğŸš€ å¤§è¦æ¨¡è™•ç†: ä½¿ç”¨ streaming-groupedï¼Œæœ€ä½³åŒ–ç®¡é“é›†æˆ")
    print("â€¢ ğŸ”— ç°¡å–®ä¸²æ¥: ä½¿ç”¨ labeledï¼Œæœ€å¤§è¨˜æ†¶é«”æ•ˆç‡")
    
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    Path(input_file).unlink(missing_ok=True)
    for result in results.values():
        Path(result["file"]).unlink(missing_ok=True)
    
    print(f"\nâœ¨ æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
